{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ca194a-43ac-41c3-85a0-517e724e2e24",
   "metadata": {},
   "source": [
    "Probability distributions for random variables and how to draw random samples from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465e2931-99dd-495d-a053-0342464f29ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.49130197 40.02356361 52.22068773 48.99138961 50.68422141 43.60135741\n",
      " 46.02351886 52.21080467 47.9772417  41.68614745]\n"
     ]
    }
   ],
   "source": [
    "# sample a normal distribution\n",
    "from numpy.random import normal\n",
    "# define the distribution\n",
    "mu = 50\n",
    "sigma = 5\n",
    "n = 10\n",
    "# generate the sample\n",
    "sample = normal(mu, sigma, n)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907762a-bc88-4a10-9fc3-1a22382ae138",
   "metadata": {},
   "source": [
    "How Bayes theorem can be used to calculate conditional probability and how it can be used in a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff6f89a-353d-4b7b-bb49-e4955ded6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb0cc56-335f-4192-b056-094e4f4b614d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# define the model\n",
    "model = GaussianNB()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# select a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cfd72ee-1a23-4294-a9b1-270a97a5f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities:  [[5.14711343e-38 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Xsample, ysample = [X[20]], y[50]\n",
    "# make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "print('Predicted Probabilities: ', yhat_prob)\n",
    "# make a classification prediction\n",
    "yhat_class = model.predict(Xsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4db72f9-1182-4b16-8a0e-9c774be51c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  [1]\n",
      "Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Class: ', yhat_class)\n",
    "print('Truth: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab948b-2daa-45e7-9a03-771bdd58daab",
   "metadata": {},
   "source": [
    "How to calculate information, entropy, and cross-entropy scores and what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d31bef-7b14-4db5-b562-8839c9bb1011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(P, Q): 3.288 bits\n",
      "H(Q, P): 2.906 bits\n"
     ]
    }
   ],
   "source": [
    "# example of calculating cross entropy\n",
    "from math import log2\n",
    " \n",
    "# calculate cross entropy\n",
    "def cross_entropy(p, q):\n",
    "\treturn -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
    " \n",
    "# define data\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "# calculate cross entropy H(P, Q)\n",
    "ce_pq = cross_entropy(p, q)\n",
    "print('H(P, Q): %.3f bits' % ce_pq)\n",
    "# calculate cross entropy H(Q, P)\n",
    "ce_qp = cross_entropy(q, p)\n",
    "print('H(Q, P): %.3f bits' % ce_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57785983-cc55-470a-943e-7e7f757a46e8",
   "metadata": {},
   "source": [
    "How to develop and evaluate the expected performance for naive classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288cb208-e3c3-4fab-894b-e585cd284fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "# example of the majority class naive classifier in scikit-learn\n",
    "from numpy import asarray\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X = asarray([0 for _ in range(100)])\n",
    "class0 = [0 for _ in range(25)]\n",
    "class1 = [1 for _ in range(75)]\n",
    "y = asarray(class0 + class1)\n",
    "# reshape data for sklearn\n",
    "X = X.reshape((len(X), 1))\n",
    "# define model\n",
    "model = DummyClassifier(strategy='most_frequent')\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# make predictions\n",
    "yhat = model.predict(X)\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y, yhat)\n",
    "print('Accuracy: %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71276c-3622-4a7c-9ff6-85eb8947b0ce",
   "metadata": {},
   "source": [
    "How to evaluate the skill of a model that predicts probability values for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "580e7002-ea7c-47ac-afa1-6f8109fd09b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24691989080446483\n"
     ]
    }
   ],
   "source": [
    "# example of log loss\n",
    "from numpy import asarray\n",
    "from sklearn.metrics import log_loss\n",
    "# define data\n",
    "y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "# define data as expected, e.g. probability for each event {0, 1}\n",
    "y_true = asarray([[v, 1-v] for v in y_true])\n",
    "y_pred = asarray([[v, 1-v] for v in y_pred])\n",
    "# calculate log loss\n",
    "loss = log_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04f14f9a-3438-42d5-88c6-f79e4c50bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05700000000000001\n"
     ]
    }
   ],
   "source": [
    "# example of brier loss\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# define data\n",
    "y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "# calculate brier score\n",
    "score = brier_score_loss(y_true, y_pred, pos_label=1)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
